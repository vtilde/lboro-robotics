{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76feeff0-fa32-4a84-b782-62ff8b6980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.yolo_model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "        self.classes = [0] # humans\n",
    "        self.tracked_id = None\n",
    "        self.last_position = None\n",
    "        self.lost = False\n",
    "        self.ignore_ids = []\n",
    "\n",
    "        self.redetect_within = 50\n",
    "\n",
    "    def show_all_boxes(self, image):\n",
    "        image = image[:, :, :-1] # remove A channel from frame\n",
    "        result = self.yolo_model.track(image, persist=True, classes=self.classes, verbose=False)[0]\n",
    "        return self.np_to_jpeg(result.plot())\n",
    "\n",
    "\n",
    "    def track(self, image):\n",
    "        self.result = self.yolo_model.track(image[:, :, :-1], persist=True, classes=self.classes, verbose=False)[0]\n",
    "        if not self.lost:\n",
    "            tracked_index = self._get_tracked_index()\n",
    "            \n",
    "            if tracked_index is False:\n",
    "                # set as lost if tracked id not found\n",
    "                self.lost = True\n",
    "                return False\n",
    "            else:\n",
    "                # add other detected objects to list of ids to ignore\n",
    "                for i in self.result.boxes.id:\n",
    "                    if i != self.tracked_id:\n",
    "                        self.ignore_ids.append(i)\n",
    "                \n",
    "                self.last_position = self.result.boxes.xyxy[tracked_index]\n",
    "                return self.last_position\n",
    "        else:\n",
    "            # if lost, wait for object with similar position to reappear\n",
    "            if self.result.boxes.id is None:\n",
    "                return False\n",
    "            \n",
    "            for i in range(len(self.result.boxes.id)):\n",
    "\n",
    "                # ignore objects that appeared at the same time as original tracked object\n",
    "                if self.result.boxes.id[i] not in self.ignore_ids:\n",
    "\n",
    "                    coords = self.result.boxes.xyxy[i]\n",
    "                    print(\"new box\", coords, self.last_position)\n",
    "                    # TODO: not redetecting, make it more lenient\n",
    "                    if (abs(coords[0] - self.last_position[0]) <= self.redetect_within\n",
    "                        and abs(coords[1] - self.last_position[1]) <= self.redetect_within\n",
    "                        and abs(coords[2] - self.last_position[2]) <= self.redetect_within\n",
    "                        and abs(coords[3] - self.last_position[3]) <= self.redetect_within\n",
    "                    ):\n",
    "                        self.lost = False\n",
    "                        return self.track(image)\n",
    "\n",
    "            # if no suitable objects found\n",
    "            return False\n",
    "\n",
    "        \n",
    "\n",
    "    def _get_tracked_index(self):\n",
    "        if self.result.boxes.id is None:\n",
    "            return False\n",
    "        if self.tracked_id in self.result.boxes.id:\n",
    "            return np.where(self.result.boxes.id.numpy() == self.tracked_id)[0][0]\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def np_to_jpeg(self, data):\n",
    "        return bytes(cv2.imencode('.jpg', data)[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da4234ca-7562-4b04-94c9-98749d6d8faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1697feb113e34c7787431ce71072c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66289bb6c0d645bdba883e7747b6c773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-19 12:38:34 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-19 12:38:34 UTC][ZED][INFO] Logging level INFO\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[2025-03-19 12:38:34 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-19 12:38:35 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-19 12:38:36 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-19 12:38:36 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-19 12:38:36 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-19 12:38:36 UTC][ZED][INFO] [Init]  Serial Number: S/N 37413003\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[03/19/2025-12:38:36] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[03/19/2025-12:38:36] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/19/2025-12:38:36] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/19/2025-12:38:36] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 11, GPU 673 (MiB)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter id to track (or leave blank to skip): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking object id 1\n",
      "new box tensor([ 11.1339, 116.2869,  45.6882, 138.8621]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 11.3944, 116.2871,  46.3835, 138.8620]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 11.1372, 116.3819,  46.0622, 138.7714]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 11.0498, 116.3794,  45.8683, 138.7720]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 10.8375, 116.3586,  45.4590, 138.7888]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.3832,   0.5751, 186.0770, 275.5199]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.5353,   0.9048, 214.1363, 277.5294]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.5027,   1.1196, 244.9250, 278.4709]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.3911,   0.6284, 289.6224, 276.8437]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 10.7241,   0.8250, 319.4255, 277.6057]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 21.2845,   1.0161, 325.1936, 278.4272]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 27.4089,   1.1069, 322.8515, 278.4625]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 28.2534,   0.7447, 318.3631, 278.0393]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 28.0206,   0.6411, 315.3558, 277.7376]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.7894, 116.4252,  45.3500, 138.7379]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 27.0951,   0.5256, 314.0410, 277.7321]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.8288, 116.3756,  45.5748, 138.7743]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 26.4504,   0.4823, 312.2088, 277.7496]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.6919, 116.3899,  45.6230, 138.7457]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 25.8468,   0.6489, 310.5347, 277.5689]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.9585, 116.4088,  45.6053, 138.7250]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 16.5187,   0.8326, 309.5975, 277.2693]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.6713, 116.3519,  45.2353, 138.6670]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  5.0245,   0.9318, 280.0119, 277.2826]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  9.4495, 116.4955,  45.2422, 138.6327]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.8814,   1.0626, 229.3098, 277.8458]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.3651,   1.2149, 207.8829, 279.0128]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.0000,   1.0737, 208.2570, 279.0354]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([1.7195e-01, 1.2510e+00, 2.0816e+02, 2.7894e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([4.9168e-02, 9.7774e-01, 1.8404e+02, 2.7869e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.3866,   0.9475, 158.7707, 279.7592]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  0.3663,   1.0658, 152.2791, 282.7748]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([2.5092e-01, 9.6689e-01, 1.5491e+02, 2.8456e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 12.2096,   0.9363, 148.1138, 285.6093]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  5.7121,   0.6030, 127.1266, 284.4608]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([1.2168e-01, 8.4294e-01, 8.6101e+01, 2.7461e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([1.0638e-01, 1.1893e+00, 6.4326e+01, 2.7489e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([1.8224e-01, 7.4678e-01, 1.1249e+02, 2.8147e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([6.6470e-02, 9.9457e-01, 1.4361e+02, 2.8674e+02]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  5.1069,   1.0371, 183.5317, 288.3835]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([  8.2796,   1.4099, 239.0329, 288.2136]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 10.3991,   1.2723, 268.8794, 288.0414]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 38.5906,   1.5746, 292.8545, 284.0542]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 54.8973,   8.6028, 296.8318, 282.1204]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 57.5461,  20.4654, 281.4286, 282.0099]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 58.5852,  30.3170, 283.9805, 281.0316]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([104.6867,  46.6165, 299.1562, 268.6509]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([140.2652,  63.5601, 313.2699, 261.0878]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([157.9679,  69.8967, 320.2393, 261.6175]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([183.5142,  71.1508, 322.2143, 262.5462]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 11.3876, 115.2706,  47.3876, 138.8684]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([197.4775,  73.1199, 324.3517, 263.0393]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 11.2716, 115.1298,  47.0545, 138.9625]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([203.0159,  74.0405, 325.0215, 263.2824]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n",
      "new box tensor([ 10.6199, 114.9901,  46.7185, 138.6330]) tensor([  0.0000,   0.5776,  78.6857, 256.5381])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m camera\u001b[38;5;241m.\u001b[39mretrieve_image(image_mat)\n\u001b[1;32m     52\u001b[0m image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[0;32m---> 54\u001b[0m tracked_box \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, image)[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[63], line 25\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myolo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlost:\n\u001b[1;32m     27\u001b[0m         tracked_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracked_index()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:607\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    606\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:268\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/predict.py:40\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/predict.py:54\u001b[0m, in \u001b[0;36mDetectionPredictor.construct_results\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Constructs a list of result objects from the predictions.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m        (list): List of result objects containing the original images, image paths, class names, and bounding boxes.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_result(pred, img, orig_img, img_path)\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pred, orig_img, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, orig_imgs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     57\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/predict.py:55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Constructs a list of result objects from the predictions.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m        (list): List of result objects containing the original images, image paths, class names, and bounding boxes.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pred, orig_img, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, orig_imgs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     57\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/predict.py:72\u001b[0m, in \u001b[0;36mDetectionPredictor.construct_result\u001b[0;34m(self, pred, img, orig_img, img_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, img, orig_img, img_path):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    Constructs the result object from the prediction.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        (Results): The result object containing the original image, image path, class names, and bounding boxes.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     pred[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Results(orig_img, path\u001b[38;5;241m=\u001b[39mimg_path, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames, boxes\u001b[38;5;241m=\u001b[39mpred[:, :\u001b[38;5;241m6\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:127\u001b[0m, in \u001b[0;36mscale_boxes\u001b[0;34m(img1_shape, boxes, img0_shape, ratio_pad, padding, xywh)\u001b[0m\n\u001b[1;32m    125\u001b[0m         boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m pad[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# y padding\u001b[39;00m\n\u001b[1;32m    126\u001b[0m boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m gain\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclip_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg0_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:347\u001b[0m, in \u001b[0;36mclip_boxes\u001b[0;34m(boxes, shape)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03mTakes a list of bounding boxes and a shape (height, width) and clips the bounding boxes to the shape.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    (torch.Tensor | numpy.ndarray): The clipped boxes.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(boxes, torch\u001b[38;5;241m.\u001b[39mTensor):  \u001b[38;5;66;03m# faster individually (WARNING: inplace .clamp_() Apple MPS bug)\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# x1\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, shape[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# y1\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, shape[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# x2\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "# initialize model\n",
    "m = Model()\n",
    "\n",
    "# get initial image and choose object to track\n",
    "image_mat = sl.Mat()\n",
    "started_tracking = False\n",
    "while not started_tracking:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        \n",
    "        image_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "        user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "        if user_input == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            m.tracked_id = int(user_input)\n",
    "            print(\"tracking object id \" + str(user_input))\n",
    "            started_tracking = True\n",
    "\n",
    "# start tracking\n",
    "running = True\n",
    "while running:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "\n",
    "        tracked_box = m.track(image)\n",
    "        if tracked_box is False:\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "        else:\n",
    "            image_rect = cv2.rectangle(\n",
    "                image,\n",
    "                (int(tracked_box[0]), int(tracked_box[1])),\n",
    "                (int(tracked_box[2]), int(tracked_box[3])),\n",
    "                (255, 0, 0),\n",
    "                4\n",
    "            )\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "        full_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "\n",
    "camera.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
