{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76feeff0-fa32-4a84-b782-62ff8b6980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.yolo_model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "        self.classes = [0] # humans\n",
    "        self.track_id = False\n",
    "        self.result = False\n",
    "        \n",
    "    def analyze(self, frame):\n",
    "        \"\"\"\n",
    "        analyzes frame and saves result to class\n",
    "        get result info using get_image, get_\n",
    "        \n",
    "        params\n",
    "        frame (np.array) : RGBA image\n",
    "\n",
    "        returns (bool) : False if no objects were detected, True otherwise\n",
    "        \"\"\"\n",
    "        frame = frame[:, :, :-1] # remove A channel from fram\n",
    "        self.result = self.yolo_model.track(frame, persist=True, classes=self.classes, verbose=False)[0]\n",
    "        \n",
    "        \n",
    "        return self.result.boxes.id != None\n",
    "\n",
    "    def track(self, track_id):\n",
    "        \"\"\"\n",
    "        start tracking specified id\n",
    "        \n",
    "        params\n",
    "        track_id (int) : numerical id of object to track\n",
    "        \"\"\"\n",
    "        self.track_id = track_id\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_tracking_index(self):\n",
    "        \"\"\"\n",
    "        finds index of tracked id in results object\n",
    "        raise exception if not found\n",
    "\n",
    "        returns (int) : index of tracked object\n",
    "        \"\"\"\n",
    "        if self.track_id in self.result.boxes.id:\n",
    "            return np.where(self.result.boxes.id.numpy() == self.track_id)[0][0]\n",
    "        else:\n",
    "            raise Exception(\"tracked id not found!!\")\n",
    "\n",
    "    \n",
    "    def get_box(self):\n",
    "        \"\"\"\n",
    "        returns coordinates of detected bounding box (top left, bottom right)\n",
    "        \n",
    "        returns ((int, int), (int, int)) : tuple of pairs of coordinates\n",
    "        \"\"\"\n",
    "        object_index = self._get_tracking_index()\n",
    "        xyxy = self.result.boxes.xyxy[object_index]\n",
    "        return (\n",
    "            (int(xyxy[0]), int(xyxy[1])),\n",
    "            (int(xyxy[2]), int(xyxy[3]))\n",
    "        )\n",
    "\n",
    "    def get_box_centre(self):\n",
    "        \"\"\"\n",
    "        get coordinates for centre of detected bounding box\n",
    "        \n",
    "        returns (int, int)\n",
    "        \"\"\"\n",
    "        box_coords = self.get_box()\n",
    "        return (\n",
    "            int((box_coords[0][0] + box_coords[1][0]) / 2),\n",
    "            int((box_coords[0][1] + box_coords[1][1]) / 2)\n",
    "        )\n",
    "    \n",
    "    def get_box_image(self, colour=(255, 0, 0), thickness=4):\n",
    "        \"\"\"\n",
    "        params\n",
    "        colour (int, int, int) : BGR colour of box \n",
    "        thickness (int) : thickness of box line\n",
    "        \n",
    "        returns (bytes) : jpeg of frame with 1 bounding box around tracked object\n",
    "        \"\"\"\n",
    "        box_coords = self.get_box()\n",
    "        annotated_image = cv2.rectangle(\n",
    "            self.result.orig_img.copy(),\n",
    "            box_coords[0],\n",
    "            box_coords[1],\n",
    "            colour,\n",
    "            thickness\n",
    "        )\n",
    "        return self.np_to_jpeg(annotated_image)\n",
    "\n",
    "\n",
    "    def get_all_boxes_image(self):\n",
    "        \"\"\"\n",
    "        returns image with all detected bounding boxes + their id and confidence\n",
    "        \n",
    "        returns (bytes) : jpeg of frame with detected bounding boxes \n",
    "        \"\"\"\n",
    "            \n",
    "        return self.np_to_jpeg(self.result.plot())\n",
    "    \n",
    "    def np_to_jpeg(self, data):\n",
    "        return bytes(cv2.imencode('.jpg', data)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c924408-1da3-4149-8118-7a4144063a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5148eaf947f4e8bbeffadb1be899194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83a805b4002454d9e85ed1b0adf53f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-19 11:31:53 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-19 11:31:53 UTC][ZED][INFO] Logging level INFO\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[2025-03-19 11:31:54 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-19 11:31:54 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-19 11:31:55 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-19 11:31:55 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-19 11:31:55 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-19 11:31:55 UTC][ZED][INFO] [Init]  Serial Number: S/N 37413003\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[03/19/2025-11:31:56] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[03/19/2025-11:31:56] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/19/2025-11:31:56] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/19/2025-11:31:56] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 18, GPU 1094 (MiB)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking object id 4\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "tracked id not found!!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     67\u001b[0m m\u001b[38;5;241m.\u001b[39manalyze(image)\n\u001b[0;32m---> 68\u001b[0m image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_box_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m full_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mget_all_boxes_image()\n\u001b[1;32m     70\u001b[0m location_coords \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mget_box_centre()\n",
      "Cell \u001b[0;32mIn[34], line 87\u001b[0m, in \u001b[0;36mModel.get_box_image\u001b[0;34m(self, colour, thickness)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_box_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, colour\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    params\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    colour (int, int, int) : BGR colour of box \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    returns (bytes) : jpeg of frame with 1 bounding box around tracked object\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     box_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     annotated_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mrectangle(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39morig_img\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m     90\u001b[0m         box_coords[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m         thickness\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_to_jpeg(annotated_image)\n",
      "Cell \u001b[0;32mIn[34], line 60\u001b[0m, in \u001b[0;36mModel.get_box\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_box\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    returns coordinates of detected bounding box (top left, bottom right)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    returns ((int, int), (int, int)) : tuple of pairs of coordinates\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     object_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracking_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     xyxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy[object_index]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     63\u001b[0m         (\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m1\u001b[39m])),\n\u001b[1;32m     64\u001b[0m         (\u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(xyxy[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m     65\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[34], line 51\u001b[0m, in \u001b[0;36mModel._get_tracking_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_id)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracked id not found!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: tracked id not found!!"
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import motors\n",
    "robot = motors.MotorsYukon()\n",
    "\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "# initialize model\n",
    "m = Model()\n",
    "\n",
    "# get initial image and choose object to track\n",
    "image_mat = sl.Mat()\n",
    "started_tracking = False\n",
    "while not started_tracking:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        \n",
    "        if m.analyze(image):\n",
    "            image_display.value = m.get_all_boxes_image()\n",
    "            \n",
    "            user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "            if user_input == \"\":\n",
    "                continue\n",
    "            else:\n",
    "                m.track(int(user_input))\n",
    "                print(\"tracking object id \" + str(user_input))\n",
    "                started_tracking = True\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "# params for turning\n",
    "from_edge = 250\n",
    "left = from_edge\n",
    "right = 672 - from_edge\n",
    "turn_speed = 0.5\n",
    "\n",
    "# start tracking\n",
    "running = True\n",
    "while running:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "\n",
    "        m.analyze(image)\n",
    "        image_display.value = m.get_box_image()\n",
    "        full_display.value = m.get_all_boxes_image()\n",
    "        location_coords = m.get_box_centre()\n",
    "        location_coords_display.value = str(location_coords)\n",
    "\n",
    "        if location_coords[0] < left:\n",
    "            robot.left(turn_speed)\n",
    "        elif location_coords[0] > right:\n",
    "            robot.right(turn_speed)\n",
    "        else:\n",
    "            robot.stop()\n",
    "        \n",
    "    \n",
    "\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d26ac8d-fac3-4623-baf1-f8b0694a1f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m.result.orig_img\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
