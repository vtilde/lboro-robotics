{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76feeff0-fa32-4a84-b782-62ff8b6980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.yolo_model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "        self.classes = [0] # humans\n",
    "        self.tracked_id = None\n",
    "        self.last_position = None\n",
    "        self.lost = False\n",
    "        self.ignore_ids = []\n",
    "\n",
    "        self.redetect_within = 50\n",
    "\n",
    "    def show_all_boxes(self, image):\n",
    "        image = image[:, :, :3] # remove A channel from frame\n",
    "        result = self.yolo_model.track(image, persist=True, classes=self.classes, verbose=False)[0]\n",
    "        return self.np_to_jpeg(result.plot())\n",
    "\n",
    "\n",
    "    def track(self, image):\n",
    "        self.result = self.yolo_model.track(image[:, :, :3], persist=True, classes=self.classes, verbose=False)[0]\n",
    "        if not self.lost:\n",
    "            tracked_index = self._get_tracked_index()\n",
    "            \n",
    "            if tracked_index is False:\n",
    "                # set as lost if tracked id not found\n",
    "                self.lost = True\n",
    "                return False\n",
    "            else:\n",
    "                # add other detected objects to list of ids to ignore\n",
    "                for i in self.result.boxes.id:\n",
    "                    if i != self.tracked_id:\n",
    "                        self.ignore_ids.append(i)\n",
    "                \n",
    "                self.last_position = self.result.boxes.xyxy[tracked_index]\n",
    "                return self.last_position\n",
    "        else:\n",
    "            # if lost, wait for object with similar position to reappear\n",
    "            if self.result.boxes.id is None:\n",
    "                return False\n",
    "            \n",
    "            for i in range(len(self.result.boxes.id)):\n",
    "\n",
    "                # ignore objects that appeared at the same time as original tracked object\n",
    "                if self.result.boxes.id[i] not in self.ignore_ids:\n",
    "\n",
    "                    coords = self.result.boxes.xyxy[i]\n",
    "                    print(\"new box id: \" + str(self.result.boxes.id[i]))\n",
    "                    print(\"    coords: \", coords)\n",
    "                    print(\"last known: \", self.last_position)\n",
    "                    # TODO: not redetecting, make it more lenient\n",
    "                    corners = [\n",
    "                        abs(coords[0] - self.last_position[0]) <= self.redetect_within,\n",
    "                        abs(coords[1] - self.last_position[1]) <= self.redetect_within,\n",
    "                        abs(coords[2] - self.last_position[2]) <= self.redetect_within,\n",
    "                        abs(coords[3] - self.last_position[3]) <= self.redetect_within\n",
    "                    ]\n",
    "                    print(corners)\n",
    "                    print(corners.count(True))\n",
    "                    print()\n",
    "                    if corners.count(True) >= 3:\n",
    "                        self.lost = False\n",
    "                        self.tracked_id = self.result.boxes.id[i]\n",
    "                        return self.track(image)\n",
    "\n",
    "            # if no suitable objects found\n",
    "            return False\n",
    "\n",
    "        \n",
    "\n",
    "    def _get_tracked_index(self):\n",
    "        if self.result.boxes.id is None:\n",
    "            return False\n",
    "        if self.tracked_id in self.result.boxes.id:\n",
    "            return np.where(self.result.boxes.id.numpy() == self.tracked_id)[0][0]\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def np_to_jpeg(self, data):\n",
    "        return bytes(cv2.imencode('.jpg', data)[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5137b62c-7cac-4538-8c63-978ebff17560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a1bef67b3048a5bde7446264d30a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206d165326c440f4a9e87c37d4f3a335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[03/26/2025-10:00:18] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[03/26/2025-10:00:18] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/26/2025-10:00:18] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/26/2025-10:00:18] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +35, now: CPU 7, GPU 420 (MiB)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter id to track (or leave blank to skip): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking object id 1\n",
      "new box id: tensor(56.)\n",
      "    coords:  tensor([648.5623,  96.7787, 671.5542, 206.2507])\n",
      "last known:  tensor([607.0129,   1.7978, 670.5184, 212.9339])\n",
      "[tensor(True), tensor(False), tensor(True), tensor(True)]\n",
      "3\n",
      "\n",
      "new box id: tensor(57.)\n",
      "    coords:  tensor([411.2874,  94.8757, 485.8384, 198.4579])\n",
      "last known:  tensor([649.2592,  96.9442, 671.4902, 206.4091])\n",
      "[tensor(False), tensor(True), tensor(False), tensor(True)]\n",
      "2\n",
      "\n",
      "new box id: tensor(58.)\n",
      "    coords:  tensor([650.7577,  97.2940, 672.0000, 149.8568])\n",
      "last known:  tensor([649.2592,  96.9442, 671.4902, 206.4091])\n",
      "[tensor(True), tensor(True), tensor(True), tensor(False)]\n",
      "3\n",
      "\n",
      "new box id: tensor(68.)\n",
      "    coords:  tensor([632.1617,  82.9624, 671.4294, 205.7449])\n",
      "last known:  tensor([650.6702,  97.5452, 671.9061, 147.9467])\n",
      "[tensor(True), tensor(True), tensor(True), tensor(False)]\n",
      "3\n",
      "\n",
      "new box id: tensor(70.)\n",
      "    coords:  tensor([520.2321,   1.0691, 671.8786, 156.6502])\n",
      "last known:  tensor([623.7186,  85.9920, 671.2540, 206.1224])\n",
      "[tensor(False), tensor(False), tensor(True), tensor(True)]\n",
      "2\n",
      "\n",
      "new box id: tensor(70.)\n",
      "    coords:  tensor([522.1439,   1.1017, 671.5585, 143.0522])\n",
      "last known:  tensor([623.7186,  85.9920, 671.2540, 206.1224])\n",
      "[tensor(False), tensor(False), tensor(True), tensor(False)]\n",
      "1\n",
      "\n",
      "new box id: tensor(70.)\n",
      "    coords:  tensor([520.8389,   1.1277, 671.8332, 145.9018])\n",
      "last known:  tensor([623.7186,  85.9920, 671.2540, 206.1224])\n",
      "[tensor(False), tensor(False), tensor(True), tensor(False)]\n",
      "1\n",
      "\n",
      "new box id: tensor(71.)\n",
      "    coords:  tensor([591.6913,  36.9501, 671.7733, 213.0214])\n",
      "last known:  tensor([623.7186,  85.9920, 671.2540, 206.1224])\n",
      "[tensor(True), tensor(True), tensor(True), tensor(True)]\n",
      "4\n",
      "\n",
      "new box id: tensor(71.)\n",
      "    coords:  tensor([580.0834,  48.9552, 671.1302, 215.3576])\n",
      "last known:  tensor([586.7278,  46.0985, 671.1661, 213.8938])\n",
      "[tensor(True), tensor(True), tensor(True), tensor(True)]\n",
      "4\n",
      "\n",
      "new box id: tensor(71.)\n",
      "    coords:  tensor([575.7442,   6.6402, 671.4363, 215.8820])\n",
      "last known:  tensor([579.0680,  48.9442, 671.2629, 215.4833])\n",
      "[tensor(True), tensor(True), tensor(True), tensor(True)]\n",
      "4\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m\n\u001b[1;32m     25\u001b[0m     image_rect \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mrectangle(\n\u001b[1;32m     26\u001b[0m         image,\n\u001b[1;32m     27\u001b[0m         (\u001b[38;5;28mint\u001b[39m(tracked_box[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(tracked_box[\u001b[38;5;241m1\u001b[39m])),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m     image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, image_rect)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m full_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_all_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# image_display.value = bytes(cv2.imencode('.jpg', image)[1])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m, in \u001b[0;36mModel.show_all_boxes\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_all_boxes\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m     19\u001b[0m     image \u001b[38;5;241m=\u001b[39m image[:, :, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;66;03m# remove A channel from frame\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myolo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_to_jpeg(result\u001b[38;5;241m.\u001b[39mplot())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:607\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    606\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1714\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1725\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py:633\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m im\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m s, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot equal to\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m max model size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinding_addrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(im\u001b[38;5;241m.\u001b[39mdata_ptr())\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinding_addrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     y \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings[x]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_names)]\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# CoreML\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "m = Model()\n",
    "\n",
    "image_display.value = m.show_all_boxes(cv2.imread(\"img/frame99.jpg\"))\n",
    "\n",
    "user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "m.tracked_id = int(user_input)\n",
    "print(\"tracking object id \" + str(user_input))\n",
    "            \n",
    "for i in range(100, 365):\n",
    "    filename = \"img/frame\" + str(i + 1) + \".jpg\"\n",
    "    image = cv2.imread(filename)\n",
    "    tracked_box = m.track(image)\n",
    "    if tracked_box is False:\n",
    "        image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "    else:\n",
    "        image_rect = cv2.rectangle(\n",
    "            image,\n",
    "            (int(tracked_box[0]), int(tracked_box[1])),\n",
    "            (int(tracked_box[2]), int(tracked_box[3])),\n",
    "            (255, 0, 0),\n",
    "            4\n",
    "        )\n",
    "        image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "    full_display.value = m.show_all_boxes(image)\n",
    "\n",
    "    # image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4234ca-7562-4b04-94c9-98749d6d8faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb636acef9d49c8b498185ed8cd32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cda2fad66b400192cb2d93ddeee256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 09:06:45 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:06:45 UTC][ZED][INFO] Logging level INFO\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[2025-03-26 09:06:45 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:06:46 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-26 09:06:47 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-26 09:06:47 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-26 09:06:47 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-26 09:06:47 UTC][ZED][INFO] [Init]  Serial Number: S/N 34032459\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lap>=0.5.12\n",
      "  Downloading lap-0.5.12-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lap>=0.5.12) (1.23.5)\n",
      "Installing collected packages: lap\n",
      "Successfully installed lap-0.5.12\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 125.2s, installed 1 package: ['lap>=0.5.12']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[03/26/2025-09:08:57] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/26/2025-09:08:57] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/26/2025-09:08:57] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     36\u001b[0m image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mshow_all_boxes(image)\n\u001b[0;32m---> 38\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menter id to track (or leave blank to skip):\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "# initialize model\n",
    "m = Model()\n",
    "\n",
    "# get initial image and choose object to track\n",
    "image_mat = sl.Mat()\n",
    "started_tracking = False\n",
    "while not started_tracking:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        \n",
    "        image_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "        user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "        if user_input == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            m.tracked_id = int(user_input)\n",
    "            print(\"tracking object id \" + str(user_input))\n",
    "            started_tracking = True\n",
    "\n",
    "# start tracking\n",
    "running = True\n",
    "while running:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "\n",
    "        tracked_box = m.track(image)\n",
    "        if tracked_box is False:\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "        else:\n",
    "            image_rect = cv2.rectangle(\n",
    "                image,\n",
    "                (int(tracked_box[0]), int(tracked_box[1])),\n",
    "                (int(tracked_box[2]), int(tracked_box[3])),\n",
    "                (255, 0, 0),\n",
    "                4\n",
    "            )\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "        full_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee972e9a-4ee7-4550-a0d0-d10dd280fb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd98928f78e43769349f59c6d999d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5a54a438c84c9caaf24c2f7b5e07cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 09:31:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:31:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:31:16 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:31:16 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-26 09:31:17 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-26 09:31:17 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-26 09:31:17 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-26 09:31:17 UTC][ZED][INFO] [Init]  Serial Number: S/N 34032459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m err \u001b[38;5;241m=\u001b[39m camera\u001b[38;5;241m.\u001b[39mgrab()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m sl\u001b[38;5;241m.\u001b[39mERROR_CODE\u001b[38;5;241m.\u001b[39mSUCCESS:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     37\u001b[0m     image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, image)[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32mpyzed/sl.pyx:9691\u001b[0m, in \u001b[0;36mpyzed.sl.Camera.retrieve_image\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:359\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    classes/types should always be True.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, qualname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Either returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    `type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "image_mat = sl.Mat()\n",
    "frame = 0\n",
    "max_frames = 10000\n",
    "while frame < max_frames:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "        # print(\"taken photo\")\n",
    "        cv2.imwrite(\"img/frame\" + str(frame) + \".jpg\", image)\n",
    "    frame += 1\n",
    "\n",
    "print(\"end\")\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c736d6-3371-4edc-9c74-39689b7d82b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
