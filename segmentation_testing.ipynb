{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a2a500-26d0-4504-9a62-fe65f9ef96ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0828a8ee20a74ea8917cd8146c3e3710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce4414866c42c4b3a9d1d222bd182e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 10:11:49 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 10:11:49 UTC][ZED][INFO] Logging level INFO\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "[2025-03-26 10:11:50 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 10:11:50 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-26 10:11:51 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-26 10:11:51 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-26 10:11:51 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-26 10:11:51 UTC][ZED][INFO] [Init]  Serial Number: S/N 34032459\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[03/26/2025-10:11:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[03/26/2025-10:11:52] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[03/26/2025-10:11:52] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[03/26/2025-10:11:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 4, GPU 252 (MiB)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter id to track (or leave blank to skip): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking object id 1\n",
      "new box id: tensor(18.)\n",
      "    coords:  tensor([534.3984,  19.0826, 671.9000, 272.2451])\n",
      "last known:  tensor([446.5999,   1.3329, 645.8844, 241.3712])\n",
      "[tensor(False), tensor(True), tensor(True), tensor(True)]\n",
      "3\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m camera\u001b[38;5;241m.\u001b[39mretrieve_image(image_mat)\n\u001b[1;32m     55\u001b[0m image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[0;32m---> 57\u001b[0m tracked_box \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     image_display\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(cv2\u001b[38;5;241m.\u001b[39mimencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, image)[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/notebooks/track_object.py:25\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myolo_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlost:\n\u001b[1;32m     27\u001b[0m         tracked_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracked_index()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:607\u001b[0m, in \u001b[0;36mModel.track\u001b[0;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[1;32m    606\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:560\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:269\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_predict_postprocess_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n\u001b[1;32m    272\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(im0s)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:406\u001b[0m, in \u001b[0;36mBasePredictor.run_callbacks\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"Runs all registered callbacks for a specific event.\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(event, []):\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/trackers/track.py:81\u001b[0m, in \u001b[0;36mon_predict_postprocess_end\u001b[0;34m(predictor, persist)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(det) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tracks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/trackers/byte_tracker.py:333\u001b[0m, in \u001b[0;36mBYTETracker.update\u001b[0;34m(self, results, img)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_predict(strack_pool)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     warp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(strack_pool, warp)\n\u001b[1;32m    335\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(unconfirmed, warp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/trackers/utils/gmc.py:115\u001b[0m, in \u001b[0;36mGMC.apply\u001b[0;34m(self, raw_frame, detections)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_ecc(raw_frame)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparseOptFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_sparseoptflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/trackers/utils/gmc.py:343\u001b[0m, in \u001b[0;36mGMC.apply_sparseoptflow\u001b[0;34m(self, raw_frame)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m H\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Find correspondences\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m matchedKeypoints, status, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcOpticalFlowPyrLK\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevFrame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevKeyPoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Leave good correspondences only\u001b[39;00m\n\u001b[1;32m    346\u001b[0m prevPoints \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from camera\n",
    "import cv2\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "# initialize model\n",
    "import track_object\n",
    "m = track_object.Model()\n",
    "\n",
    "# get initial image and choose object to track\n",
    "image_mat = sl.Mat()\n",
    "started_tracking = False\n",
    "while not started_tracking:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        \n",
    "        image_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "        user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "        if user_input == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            m.tracked_id = int(user_input)\n",
    "            print(\"tracking object id \" + str(user_input))\n",
    "            started_tracking = True\n",
    "\n",
    "# start tracking\n",
    "running = True\n",
    "while running:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "\n",
    "        tracked_box = m.track(image)\n",
    "        if tracked_box is False:\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "        else:\n",
    "            image_rect = cv2.rectangle(\n",
    "                image,\n",
    "                (int(tracked_box[0]), int(tracked_box[1])),\n",
    "                (int(tracked_box[2]), int(tracked_box[3])),\n",
    "                (255, 0, 0),\n",
    "                4\n",
    "            )\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "        full_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "\n",
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829d0da-6809-49ee-8dea-9f4005dd1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from images\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "m = Model()\n",
    "\n",
    "image_display.value = m.show_all_boxes(cv2.imread(\"img/frame99.jpg\"))\n",
    "\n",
    "user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "m.tracked_id = int(user_input)\n",
    "print(\"tracking object id \" + str(user_input))\n",
    "\n",
    "for i in range(100, 365):\n",
    "    filename = \"img/frame\" + str(i + 1) + \".jpg\"\n",
    "    image = cv2.imread(filename)\n",
    "    tracked_box = m.track(image)\n",
    "    if tracked_box is False:\n",
    "        image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "    else:\n",
    "        image_rect = cv2.rectangle(\n",
    "            image,\n",
    "            (int(tracked_box[0]), int(tracked_box[1])),\n",
    "            (int(tracked_box[2]), int(tracked_box[3])),\n",
    "            (255, 0, 0),\n",
    "            4\n",
    "        )\n",
    "        image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "    full_display.value = m.show_all_boxes(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782684db-c15e-4a84-b1d1-1378b65f75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "seg_model = YOLO(\"yolo11l-seg.pt\")\n",
    "seg_model.export(format=\"engine\", half=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b3772b-7cb9-4b83-9e05-5da3143cbc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cecbacd92046c286249df593f4b85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11l-seg.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "WARNING ⚠️ Metadata not found for 'model=yolo11l-seg.onnx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-26 12:03:26.873848247 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "id : 1\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class SegModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.yolo_model = YOLO(\"yolo11l_half.engine\")\n",
    "        self.seg_model = YOLO(\"yolo11l-seg.onnx\")\n",
    "        \n",
    "        self.classes = [0] # humans\n",
    "        self.tracked_id = None # remember to set before running self.track()\n",
    "        self.last_position = None\n",
    "        self.lost = False\n",
    "        self.ignore_ids = []\n",
    "\n",
    "        self.redetect_within = 50\n",
    "\n",
    "    def show_all_boxes(self, image):\n",
    "        \"\"\"\n",
    "        Get a jpg image of the frame with all detected bounding boxes with their ids\n",
    "        To be used at the start to select the object to track\n",
    "\n",
    "        params\n",
    "        image (np.array) : BGR(A) array of image to analyze (From pyzed.sl.Mat().get_data())\n",
    "\n",
    "        returns\n",
    "        (bytes) jpg image with bounding boxes + ids\n",
    "        \"\"\"\n",
    "        image = image[:, :, :3] # remove A channel from frame\n",
    "        result = self.yolo_model.track(image, persist=True, classes=self.classes, verbose=False)[0]\n",
    "        return self.np_to_jpeg(result.plot())\n",
    "\n",
    "\n",
    "    def track(self, image):\n",
    "        \"\"\"\n",
    "        Run the model on an image to keep track of and locate the tracked object\n",
    "        If the tracked object disappeared from frame, will change mode to \"lost\" and will analyze new frames for any new object that:\n",
    "        - Was not seen at the same time as the original object\n",
    "        - Has a bounding box with similar location to the original object's last known position\n",
    "        While lost, will return the input image with no bounding box\n",
    "        \n",
    "        When a matching object is detected, it will resume tracking that object and return the image with a bounding box\n",
    "\n",
    "        params\n",
    "        image (np.array) : BGR(A) array of image to analyze (From pyzed.sl.Mat().get_data())\n",
    "\n",
    "        returns\n",
    "        (bytes) jpg image with bounding box of tracked object\n",
    "        \"\"\"\n",
    "        self.result = self.yolo_model.track(image[:, :, :3], persist=True, classes=self.classes, verbose=False)[0]\n",
    "        if not self.lost:\n",
    "            tracked_index = self._get_tracked_index()\n",
    "            \n",
    "            if tracked_index is False:\n",
    "                # change mode to lost if tracked id not found\n",
    "                self.lost = True\n",
    "                return False\n",
    "            else:\n",
    "                # add other detected objects to list of ids to ignore (in case of tracked object lost)\n",
    "                for i in self.result.boxes.id:\n",
    "                    if i != self.tracked_id:\n",
    "                        self.ignore_ids.append(i)\n",
    "                \n",
    "                self.last_position = self.result.boxes.xyxy[tracked_index]\n",
    "                return self.last_position\n",
    "        else:\n",
    "            # if lost, wait for object with similar position to reappear\n",
    "            if self.result.boxes.id is None:\n",
    "                return False\n",
    "            \n",
    "            for i in range(len(self.result.boxes.id)):\n",
    "\n",
    "                # ignore objects that appeared at the same time as original tracked object\n",
    "                if self.result.boxes.id[i] not in self.ignore_ids:\n",
    "\n",
    "                    coords = self.result.boxes.xyxy[i]\n",
    "                    # print(\"new box id: \" + str(self.result.boxes.id[i]))\n",
    "                    # print(\"    coords: \", coords)\n",
    "                    # print(\"last known: \", self.last_position)\n",
    "                    corners = [\n",
    "                        abs(coords[0] - self.last_position[0]) <= self.redetect_within,\n",
    "                        abs(coords[1] - self.last_position[1]) <= self.redetect_within,\n",
    "                        abs(coords[2] - self.last_position[2]) <= self.redetect_within,\n",
    "                        abs(coords[3] - self.last_position[3]) <= self.redetect_within\n",
    "                    ]\n",
    "                    # print(corners)\n",
    "                    # print(corners.count(True))\n",
    "                    # print()\n",
    "                    \n",
    "                    # assume the object is the original if at least 3 corners are close to the last known position\n",
    "                    if corners.count(True) >= 3:\n",
    "                        self.lost = False\n",
    "                        self.tracked_id = self.result.boxes.id[i]\n",
    "                        return self.track(image)\n",
    "\n",
    "            # if no suitable objects found\n",
    "            return False\n",
    "            \n",
    "    def segment(self, image):\n",
    "        self.seg_result = self.seg_model.track(image[:, :, :3], persist=True, classes=self.classes, verbose=False)[0]\n",
    "        return self.seg_result.plot()\n",
    "\n",
    "    def get_seg_masks(self, seg_id):\n",
    "        for i in range(len(self.seg_result.boxes)):\n",
    "            if self.seg_result.boxes.id[i] == seg_id:\n",
    "                return self.seg_result.masks.xy[i]\n",
    "\n",
    "    def _get_tracked_index(self):\n",
    "        if self.result.boxes.id is None:\n",
    "            return False\n",
    "        if self.tracked_id in self.result.boxes.id:\n",
    "            return np.where(self.result.boxes.id.numpy() == self.tracked_id)[0][0]\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def np_to_jpeg(self, data):\n",
    "        return bytes(cv2.imencode('.jpg', data)[1])\n",
    "\n",
    "\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "display_1 = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display_2 = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([display_1, display_2]))\n",
    "\n",
    "import cv2\n",
    "\n",
    "ms = SegModel()\n",
    "img = ms.segment(cv2.imread(\"img/frame100.jpg\"))\n",
    "display_1.value = ms.np_to_jpeg(img)\n",
    "points = ms.get_seg_masks(int(input(\"id :\")))\n",
    "for i in points:\n",
    "    img = cv2.circle(\n",
    "        img,\n",
    "        (int(i[0]), int(i[1])),\n",
    "        1,\n",
    "        (0, 0, 255),\n",
    "        2\n",
    "    )\n",
    "display_2.value = ms.np_to_jpeg(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673e9388-daf7-400d-8db6-21cfc7880ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507.15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9309110-e8fa-4985-8492-84cf07f67ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11l-seg.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "WARNING ⚠️ Metadata not found for 'model=yolo11l-seg.onnx'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-26 11:53:51.191936871 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 4 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'plot_im'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ms \u001b[38;5;241m=\u001b[39m SegModel()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg/frame100.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m, in \u001b[0;36mSegModel.get_segment\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_segment\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_model\u001b[38;5;241m.\u001b[39mtrack(image[:, :, :\u001b[38;5;241m3\u001b[39m], persist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseg_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_im\u001b[49m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/__init__.py:239\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Results' object has no attribute 'plot_im'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    This class encapsulates the functionality for handling detection, segmentation, pose estimation,\n    and classification results from YOLO models.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (Tuple[int, int]): Original image shape in (height, width) format.\n        boxes (Boxes | None): Object containing detection bounding boxes.\n        masks (Masks | None): Object containing detection masks.\n        probs (Probs | None): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints | None): Object containing detected keypoints for each object.\n        obb (OBB | None): Object containing oriented bounding boxes.\n        speed (Dict[str, float | None]): Dictionary of preprocess, inference, and postprocess speeds.\n        names (Dict[int, str]): Dictionary mapping class IDs to class names.\n        path (str): Path to the image file.\n        _keys (Tuple[str, ...]): Tuple of attribute names for internal use.\n\n    Methods:\n        update: Updates object attributes with new detection results.\n        cpu: Returns a copy of the Results object with all tensors on CPU memory.\n        numpy: Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda: Returns a copy of the Results object with all tensors on GPU memory.\n        to: Returns a copy of the Results object with tensors on a specified device and dtype.\n        new: Returns a new Results object with the same image, path, and names.\n        plot: Plots detection results on an input image, returning an annotated image.\n        show: Shows annotated results on screen.\n        save: Saves annotated results to file.\n        verbose: Returns a log string for each task, detailing detections and classifications.\n        save_txt: Saves detection results to a text file.\n        save_crop: Saves cropped detection images.\n        tojson: Converts detection results to JSON format.\n\n    Examples:\n        >>> results = model(\"path/to/image.jpg\")\n        >>> for result in results:\n        ...     print(result.boxes)  # Print detection boxes\n        ...     result.show()  # Display the annotated image\n        ...     result.save(filename=\"result.jpg\")  # Save annotated image\n    "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
