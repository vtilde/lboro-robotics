{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0f9cea-a074-4f0e-bad0-e465316ad2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23395f969454e048126cd8357021e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ca88cd166c44bfac5337bcae719417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 11:15:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 11:15:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 11:15:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-04-30 11:15:11 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-04-30 11:15:11 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-04-30 11:15:11 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-04-30 11:15:11 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-04-30 11:15:11 UTC][ZED][INFO] [Init]  Serial Number: S/N 35159485\n",
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[04/30/2025-12:15:12] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[04/30/2025-12:15:12] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[04/30/2025-12:15:12] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[04/30/2025-12:15:12] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 10, GPU 589 (MiB)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): \n",
      "enter id to track (or leave blank to skip): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking object id 1\n",
      "horizontal centre: 357\n",
      "horizontal centre: 357\n",
      "horizontal centre: 356\n",
      "horizontal centre: 359\n",
      "horizontal centre: 359\n",
      "horizontal centre: 353\n",
      "horizontal centre: 350\n",
      "horizontal centre: 349\n",
      "horizontal centre: 347\n",
      "horizontal centre: 347\n",
      "horizontal centre: 351\n",
      "horizontal centre: 368\n",
      "horizontal centre: 401\n",
      "horizontal centre: 432\n",
      "horizontal centre: 467\n",
      "horizontal centre: 452\n",
      "horizontal centre: 368\n",
      "horizontal centre: 295\n",
      "horizontal centre: 296\n",
      "horizontal centre: 270\n",
      "horizontal centre: 210\n",
      "horizontal centre: 179\n",
      "horizontal centre: 218\n",
      "horizontal centre: 313\n",
      "horizontal centre: 420\n",
      "horizontal centre: 423\n",
      "horizontal centre: 411\n",
      "horizontal centre: 397\n",
      "horizontal centre: 362\n",
      "horizontal centre: 337\n",
      "horizontal centre: 321\n",
      "horizontal centre: 282\n",
      "horizontal centre: 260\n",
      "horizontal centre: 255\n",
      "horizontal centre: 244\n",
      "horizontal centre: 243\n",
      "horizontal centre: 319\n",
      "horizontal centre: 382\n",
      "horizontal centre: 380\n",
      "horizontal centre: 391\n",
      "horizontal centre: 393\n",
      "horizontal centre: 394\n",
      "horizontal centre: 398\n",
      "horizontal centre: 397\n",
      "horizontal centre: 397\n",
      "horizontal centre: 397\n",
      "horizontal centre: 396\n",
      "horizontal centre: 399\n",
      "horizontal centre: 400\n",
      "horizontal centre: 400\n",
      "horizontal centre: 405\n",
      "horizontal centre: 420\n",
      "horizontal centre: 463\n",
      "horizontal centre: 506\n",
      "horizontal centre: 445\n",
      "horizontal centre: 353\n",
      "horizontal centre: 297\n",
      "horizontal centre: 300\n",
      "horizontal centre: 312\n",
      "horizontal centre: 340\n",
      "horizontal centre: 376\n",
      "horizontal centre: 443\n",
      "horizontal centre: 457\n",
      "horizontal centre: 412\n",
      "horizontal centre: 339\n",
      "horizontal centre: 347\n",
      "horizontal centre: 326\n",
      "horizontal centre: 304\n",
      "horizontal centre: 269\n",
      "horizontal centre: 195\n",
      "horizontal centre: 176\n",
      "horizontal centre: 236\n",
      "horizontal centre: 320\n",
      "horizontal centre: 419\n",
      "horizontal centre: 428\n",
      "horizontal centre: 424\n",
      "horizontal centre: 424\n",
      "horizontal centre: 426\n",
      "horizontal centre: 431\n",
      "horizontal centre: 431\n",
      "horizontal centre: 432\n",
      "horizontal centre: 448\n",
      "horizontal centre: 446\n",
      "horizontal centre: 351\n",
      "horizontal centre: 266\n",
      "horizontal centre: 287\n",
      "horizontal centre: 298\n",
      "horizontal centre: 307\n",
      "horizontal centre: 318\n",
      "horizontal centre: 315\n",
      "horizontal centre: 305\n",
      "horizontal centre: 298\n",
      "horizontal centre: 295\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m image \u001b[38;5;241m=\u001b[39m image_mat\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m     65\u001b[0m tracked_box \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mtrack(image, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorners\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m x_centre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((\u001b[43mtracked_box\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m tracked_box[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal centre: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(x_centre))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "image_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "full_display = widgets.Image(format=\"jpeg\", width=\"45%\")\n",
    "display(widgets.HBox([image_display, full_display]))\n",
    "\n",
    "location_coords_display = widgets.Label()\n",
    "display(location_coords_display)\n",
    "\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "camera = sl.Camera()\n",
    "camera_params = sl.InitParameters()\n",
    "camera_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "camera_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "camera_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "camera_status = camera.open(camera_params)\n",
    "if camera_status != sl.ERROR_CODE.SUCCESS:\n",
    "    print(\"camera error\")\n",
    "    print(camera_status)\n",
    "    camera.close()\n",
    "    exit()\n",
    "\n",
    "# initialize robot\n",
    "import motors\n",
    "robot = motors.MotorsYukon()\n",
    "robot.stop()\n",
    "\n",
    "# initialize model\n",
    "import tracking\n",
    "m = tracking.Model()\n",
    "\n",
    "# get initial image and choose object to track\n",
    "image_mat = sl.Mat()\n",
    "started_tracking = False\n",
    "while not started_tracking:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "        \n",
    "        image_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "        user_input = input(\"enter id to track (or leave blank to skip):\")\n",
    "        if user_input == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            m.tracked_id = int(user_input)\n",
    "            print(\"tracking object id \" + str(user_input))\n",
    "            started_tracking = True\n",
    "\n",
    "# turning parameters\n",
    "KEEP_X_BETWEEN = (250, 432)\n",
    "TURN_SPEED = 0.3\n",
    "\n",
    "# start tracking\n",
    "running = True\n",
    "while running:\n",
    "    err = camera.grab()\n",
    "    if err == sl.ERROR_CODE.SUCCESS:\n",
    "        camera.retrieve_image(image_mat)\n",
    "        image = image_mat.get_data()\n",
    "\n",
    "        tracked_box = m.track(image, return_type=\"corners\")\n",
    "        x_centre = int((tracked_box[0] + tracked_box[2]) / 2)\n",
    "        print(\"horizontal centre: \" + str(x_centre))\n",
    "        \n",
    "        if tracked_box is False:\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image)[1])\n",
    "            robot.stop()\n",
    "        else:\n",
    "            image_rect = cv2.rectangle(\n",
    "                image,\n",
    "                (int(tracked_box[0]), int(tracked_box[1])),\n",
    "                (int(tracked_box[2]), int(tracked_box[3])),\n",
    "                (255, 0, 0),\n",
    "                4\n",
    "            )\n",
    "            image_display.value = bytes(cv2.imencode('.jpg', image_rect)[1])\n",
    "\n",
    "            # turn if tracked person is outside KEEP_X_BETWEEN\n",
    "            if x_centre < KEEP_X_BETWEEN[0]:\n",
    "                robot.left(speed=TURN_SPEED)\n",
    "            elif x_centre > KEEP_X_BETWEEN[1]:\n",
    "                robot.right(speed=TURN_SPEED)\n",
    "            else:\n",
    "                robot.stop()\n",
    "\n",
    "        full_display.value = m.show_all_boxes(image)\n",
    "    \n",
    "\n",
    "camera.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
