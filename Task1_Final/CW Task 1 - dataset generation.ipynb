{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa4c09",
   "metadata": {},
   "source": [
    "# Generating the Dataset for Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b4759",
   "metadata": {},
   "source": [
    "For task 1 (line following), the robot relies on a CNN that must be trained using a dataset of images containing the rope that it will be following. To achieve this, we capture images using the camera on the robot, crop them to only include the data we need (i.e. the rope), extract all yellow-coloured objects from the image (which we assume will only be the rope), and determine if the rope is in the centre of the image or not. We can then use this result to move the robot forward, left, or right, all whilst saving captured images, which are used to build up the dataset. In total, we have captured almost 3000 images to train our CNN on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2a7a2-1859-4348-9f2a-c524a50b25a3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93764fe1-9152-4b90-931b-389622ba23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "from IPython.display import FileLink\n",
    "import time\n",
    "import motors\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import sys\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19861d55",
   "metadata": {},
   "source": [
    "## Extracting and Centering the Rope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bc99a",
   "metadata": {},
   "source": [
    "This function is used to extract any yellow objects from the image, and return a new image with only yellow objects visible, and all others colours set to black. This simplifies the dataset for the CNN to make a prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2e840-5771-4a43-bc6c-e3ee00488f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_yellow(frame):\n",
    "    # Convert image to HSV format\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Lower and upper bounds of the colour yellow in HSV format\n",
    "    lower_bound = np.array([30,40,120])\n",
    "    upper_bound = np.array([75, 255, 255])\n",
    "\n",
    "    # Extract yellow colours\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Makes all other colours black (so only yellow is visible)\n",
    "    return cv2.bitwise_and(frame, frame, mask=mask), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582c2e0",
   "metadata": {},
   "source": [
    "The following function takes the yellow-extracted image, draws a bounding box around the largest object (which we expect to be the rope), then calculates the centre of the bounding box in the horizontal axis. Using the result of this function, we can compare the rope's centre value to the robot's centre value (using the width of its captured images) to determine whether the robot is centred on the rope or needs to re-adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4a2d4-39c5-499c-b0d9-5c1c41985626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centre_rope(frame, mask):\n",
    "    # Extract edges of object\n",
    "    contours,_ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Draw bounding box around extracted object (rope)\n",
    "        x,_,w,_ = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Calculate horizontal centre of bounding box\n",
    "        rope_centre_x = x + w // 2\n",
    "\n",
    "        return rope_centre_x, mask\n",
    "    \n",
    "    return None, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08a1af-80ca-449e-b788-3175f10b2175",
   "metadata": {},
   "source": [
    "## Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f217d5c",
   "metadata": {},
   "source": [
    "The following code is mostly sourced from the Lab Tutorials (especially Tutorial 2 Part B). This code deals with initialising the camera and the captured footage in widgets for easy monitoring and debugging. The class is extended to allow getting the last taken image so it can be saved as a .PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e3531-27a5-42b5-b750-47568f09112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304ad4f8a144411095a4a8f02a63d4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='30%'), Image(value=b'', format='jpeg', width='30%')), laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create widgets for the displaying colour and depth images\n",
    "display_color = widgets.Image(format='jpeg', width='30%')\n",
    "display_depth = widgets.Image(format='jpeg', width='30%')\n",
    "layout=widgets.Layout(width='100%')\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "class Camera():\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        # Initialise camera\n",
    "        self.zed = sl.Camera()\n",
    "        \n",
    "        # Provide initial configuration\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "\n",
    "        #Ensure the camera has opened succesfully\n",
    "        if status != sl.ERROR_CODE.SUCCESS: \n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        # Flag to control the thread\n",
    "        self.thread_running_flag = False\n",
    "\n",
    "        # Get the height and width of camera images\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "\n",
    "        self.output = None\n",
    "        self.color_value = None\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_running_flag==True): \n",
    "           \n",
    "           # Ensures camera is active and working\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "\n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "\n",
    "                # Display colour image in widget\n",
    "                self.color_value = self.image.get_data()\n",
    "                cv2.putText(self.color_value, 'o', (self.width//2,self.height//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                display_color.value = bgr8_to_jpeg(self.color_value)\n",
    "\n",
    "                # Display depth image in widget\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())\n",
    "                depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET) \n",
    "                cv2.putText(depth_colormap, str(self.depth_image[self.height//2,self.width//2]), (self.width//2,self.height//2), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                display_depth.value = bgr8_to_jpeg(depth_colormap)\n",
    "    \n",
    "    def start(self):\n",
    "        # Starts capture_frames thread\n",
    "        if self.thread_running_flag == False: \n",
    "            self.thread_running_flag=True \n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()      \n",
    "\n",
    "    def stop(self): \n",
    "        # Ends all running threads\n",
    "        if self.thread_running_flag == True:\n",
    "            self.thread_running_flag = False\n",
    "            self.thread.join()   \n",
    "            self.output.release() \n",
    "\n",
    "    def get_image(self):\n",
    "        # Gets the most recently captured image\n",
    "        image = self.color_value\n",
    "        return image\n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    # Convert numpy array to jpeg coded data for displaying in widget\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff813196-e596-4d01-9ac9-1abba478acb4",
   "metadata": {},
   "source": [
    "## Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339ed7d",
   "metadata": {},
   "source": [
    "This function determines whether the robot should move forward, left, or right, depending on whether the rope is centrally aligned with the robot (within a threshold). With each movement, an image is captured and stored in the root folder so it can be cumulated into a complete dataset for training the CNN. Each image is named with the direction the robot moved when it saw that image, which provides a very simple labelling method that hugely speeds up the creation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b6671c-a285-40ec-858b-516982466d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_robot(rope_centre_x, frame_width, image, count, threshold=50):\n",
    "    # Calculate the centre of the robot's viewpoint\n",
    "    screen_centre_x = frame_width // 2\n",
    "    \n",
    "    if rope_centre_x is None:\n",
    "        # Stop if rope is lost\n",
    "        robot.stop()\n",
    "        return\n",
    "        \n",
    "    if abs(rope_centre_x - screen_centre_x) < threshold:\n",
    "        # Continue forward if rope is approximately centred\n",
    "        robot.forward(0.2)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        # Save image\n",
    "        cv2.imwrite(f\"forward_{str(count)}.png\", image)\n",
    "        return\n",
    "        \n",
    "    elif rope_centre_x < screen_centre_x:\n",
    "        # Move left if rope is positioned to the left of the centre of the image\n",
    "        robot.left(0.1)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Save image\n",
    "        cv2.imwrite(f\"left_{str(count)}.png\", image)\n",
    "        return\n",
    "        \n",
    "    else: \n",
    "        # Move right if rope is positioned to the right of the centre of the image\n",
    "        robot.right(0.1)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Save image\n",
    "        cv2.imwrite(f\"right_{str(count)}.png\", image)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e96d0-8c88-4b40-a2e4-e83cf0d2271c",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0952b-5d13-43de-8767-41cb28951a95",
   "metadata": {},
   "source": [
    "Finally, we combine all logic implemented so far to initialise the robot and camera, then allow it to move based on the position of the rope, capturing images as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc1e96e-c5a0-44b9-aaa3-5731392236ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 09:55:38 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:55:38 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:55:38 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-03-26 09:55:39 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-03-26 09:55:40 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-03-26 09:55:40 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-03-26 09:55:40 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-03-26 09:55:40 UTC][ZED][INFO] [Init]  Serial Number: S/N 32675467\n"
     ]
    }
   ],
   "source": [
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "camera = Camera()\n",
    "camera.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f8449-33cf-47a2-84c1-7fa2ae62a297",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b786b-1317-48b4-9a4b-d212689d3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while True:\n",
    "    img = camera.get_image()\n",
    "    \n",
    "    if img is not None:\n",
    "        \n",
    "        # Crop the image \n",
    "        h, w = img.shape[:2]\n",
    "        margin_width = 200\n",
    "        margin_height = 220\n",
    "        cropped_image = img[margin_height:,margin_width:w-margin_width]\n",
    "\n",
    "        # Calculate the centre of the image\n",
    "        h, w = cropped_image.shape[:2]\n",
    "        centre_x = w // 2\n",
    "        centre_y = h // 2\n",
    "\n",
    "        # Extract yellow from cropped image\n",
    "        frame_width = cropped_image.shape[1]\n",
    "        frame, mask = extract_yellow(cropped_image)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)        \n",
    "\n",
    "        # Calculate centre of rope in image\n",
    "        rope_centre_x, mask = centre_rope(frame, mask)\n",
    "        \n",
    "        # Display result using matplotlib\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # Move the robot based on rope position\n",
    "        move_robot(rope_centre_x, frame_width, frame_rgb, count)\n",
    "\n",
    "        # Increment count to prevent images being overwritten\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da73eb-ae9a-4449-9be3-fbe0b0050301",
   "metadata": {},
   "source": [
    "## Flip left and right images to programmatically expand dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87aa7fc-2a87-4d2b-9839-5a5ec1c4820c",
   "metadata": {},
   "source": [
    "In order to quickly aquire more left and right images in the dataset, we can augment the images already captured by flipping them. In this way, left images become right images, and right images become left images. This effectively doubles the amount of left and right images in our dataset, which helps to deal with data sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ce090-8254-47ca-8037-09a29b0b1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(\"Task 1 Images/left\"):\n",
    "    # Get the image\n",
    "    left_filepath = os.path.join(\"Task 1 Images/left\", filename)\n",
    "    image = cv2.imread(left_filepath)\n",
    "    image = cv2.flip(image, 1) \n",
    "\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # Change filepath to point to Right directory\n",
    "    file_without_extension = filename[:-4]\n",
    "    filepath = os.path.join(\"Task 1 Images/right\", f\"right_{count}_flipped.png\")\n",
    "\n",
    "    # Save image\n",
    "    cv2.imwrite(filepath+'_flipped.png', image)\n",
    "\n",
    "    # Increment count to prevent overwrites\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142823e4-f46c-42c1-9fb8-f79a20f70d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for filename in os.listdir(\"Task 1 Images/right\"):\n",
    "    # Get the image\n",
    "    left_filepath = os.path.join(\"Task 1 Images/right\", filename)\n",
    "    image = cv2.imread(left_filepath)\n",
    "    image = cv2.flip(image, 1) \n",
    "\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # Change filepath to point to Left directory\n",
    "    file_without_extension = filename[:-4]\n",
    "    filepath = os.path.join(\"Task 1 Images/left\", f\"left_{count}_flipped.png\")\n",
    "\n",
    "    # Save image\n",
    "    cv2.imwrite(filepath+'_flipped.png', image)\n",
    "\n",
    "    # Increment to prevent overwrites\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a3257-363f-48a0-89b4-76e014d633d4",
   "metadata": {},
   "source": [
    "## Saving Dataset Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e273b8-ce27-4559-b5dc-1044bcd13d96",
   "metadata": {},
   "source": [
    "To upload the captured dataset to GitHub, it first needed to be downloaded on the local machine. We did this so that all team members could access the dataset, and it would persist week-after-week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248a5c5-4a4a-4ac4-b554-e98a96f04f13",
   "metadata": {},
   "source": [
    "The following code is used to append a '_2' to each image captured in the week. This was done to prevent newly captured images from overwriting images that were captured in previous Tutorials when uploading to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0734579-8b49-466f-89c9-9cc90bdae895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"Task 1 Images/left\"):\n",
    "    file_without_extension = filepath[:-4]\n",
    "    filepath = os.path.join(\"Task 1 Images/left\", filename)\n",
    "    os.rename(filepath, file_without_extension+'_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c5a12-dddb-4610-8210-1cdace4ed0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"Task 1 Images/right\"):\n",
    "    file_without_extension = filepath[:-4]\n",
    "    filepath = os.path.join(\"Task 1 Images/right\", filename)\n",
    "    os.rename(filepath, file_without_extension+'_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761aad77-6534-43cb-b9ff-44fe1113cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"Task 1 Images/forward\"):\n",
    "    file_without_extension = filepath[:-4]\n",
    "    filepath = os.path.join(\"Task 1 Images/forward\", filename)\n",
    "    os.rename(filepath, file_without_extension+'_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc9a08-4096-4d64-a5c6-c2e82f921905",
   "metadata": {},
   "source": [
    "This final piece of code compresses the dataset and presents it as a link that can be downloaded from within the notebook. This allows us to quickly download the dataset from the robots so it can be prepared for upload to GitHub. Shutil is used to compress the folder, whilst IPython FileLink is used to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549de9da-8b47-4041-9ce4-1d98e07b656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='images_cropped.zip' target='_blank'>images_cropped.zip</a><br>"
      ],
      "text/plain": [
       "/home/robotics/Documents/notebooks/images_cropped.zip"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = 'Task 1 Images'\n",
    "output_zip = 'images_cropped.zip'\n",
    "\n",
    "# Compress dataset into Zip file\n",
    "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_path)\n",
    "\n",
    "# Create download link\n",
    "FileLink(output_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
