{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Direction Classifier for Task 1\n",
    "\n",
    "This notebook implements a Convolutional Neural Network (CNN) to train a line-following robot. The model analyzes images of lines and classifies them into three directions: forward, left, or right. This classification enables the robot to make real-time navigation decisions by processing camera input to follow lines on the ground, the images were generated according to the previous notebook using the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for the CNN model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "To handle our image data efficiently, we'll create a custom PyTorch Dataset class. This class loads image paths and their corresponding labels (forward, left, right), applies transformations, and prepares them for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for loading images\n",
    "class LineDirectionDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Augmentation\n",
    "\n",
    "Before feeding images to our neural network, we need to preprocess them to ensure consistent input format. The transforms pipeline:\n",
    "\n",
    "1. **Resizes** all images to 224Ã—224 pixels (standard size for many CNN architectures)\n",
    "2. **Converts** images to PyTorch tensors\n",
    "3. **Normalizes** pixel values using ImageNet mean and standard deviation\n",
    "\n",
    "This preprocessing standardizes the input data, which helps the network learn more effectively.\n",
    "\n",
    "Next, we'll load the image dataset from the 'images' directory, which contains three subdirectories (forward, left, right) with images of each line direction captured by the robot's camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load images from directories\n",
    "image_dir = 'images'\n",
    "categories = ['forward', 'left', 'right']\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    category_path = os.path.join(image_dir, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_paths.append(os.path.join(category_path, filename))\n",
    "            labels.append(idx)  # 0=forward, 1=left, 2=right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "To properly evaluate our model, we need to separate our data into training and testing sets. We'll use 67% of the data for training and 33% for testing.\n",
    "\n",
    "The training set is used to teach the model, while the testing set allows us to evaluate how well it generalizes to new, unseen data. This validation is crucial to ensure the robot will make reliable decisions when deployed in the real world with new line patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = LineDirectionDataset(train_paths, train_labels, transform=transform)\n",
    "test_dataset = LineDirectionDataset(test_paths, test_labels, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Definition\n",
    "\n",
    "Now we define the neural network architecture for our line detection model. We're using a simple Convolutional Neural Network (CNN) with:\n",
    "\n",
    "1. A convolutional layer that extracts 64 feature maps from the input image\n",
    "2. A pooling layer to reduce spatial dimensions and computational requirements\n",
    "3. Two fully-connected layers that process these features and output directional predictions\n",
    "\n",
    "This simple architecture is sufficient for this task and can run efficiently on the robot's hardware. For a production robot, you might consider a more optimized architecture like MobileNet that balances accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for direction classification\n",
    "class LineDirectionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LineDirectionCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 112 * 112, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # 3 output classes (forward, left, right)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 64 * 112 * 112)  # Flatten the image tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Setup\n",
    "\n",
    "With our architecture defined, we'll initialize the model and set up the training environment:\n",
    "\n",
    "1. Create an instance of our CNN model\n",
    "2. Set up GPU acceleration if available (speeds up training significantly)\n",
    "3. Define our loss function (Cross-Entropy Loss is standard for classification)\n",
    "4. Configure the Adam optimizer with a learning rate of 0.0015\n",
    "\n",
    "These configurations provide a good balance between training speed and model accuracy for our robot's direction classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = LineDirectionCNN()\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we'll train our CNN model on the dataset of line images. During training, the model:\n",
    "\n",
    "1. Processes batches of 64 images at a time\n",
    "2. Makes predictions about the direction (forward, left, or right)\n",
    "3. Compares these predictions to the true labels\n",
    "4. Updates its internal parameters to minimize the prediction error\n",
    "\n",
    "We'll train for 15 epochs (complete passes through the dataset) and track both loss and accuracy to monitor the learning progress. As training proceeds, we should see the loss decrease and the accuracy increase, indicating that the robot is learning to classify directions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move inputs and labels to the selected device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct_predictions/total_predictions:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training, we need to evaluate our model's performance on the test set - images the model hasn't seen during training. This evaluation provides a realistic estimate of how well our robot will perform in real-world line-following scenarios.\n",
    "\n",
    "A high test accuracy indicates that the robot should be able to reliably detect line directions when deployed. We'd typically aim for at least 90% accuracy for reliable robot navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on test data\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the selected device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "print(f'Test Accuracy: {correct_predictions/total_predictions:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Prediction Function\n",
    "\n",
    "To understand how our robot \"sees\" and interprets line directions, we'll create a visualization function. This function:\n",
    "\n",
    "1. Takes a sample image from our test set\n",
    "2. Processes it through our trained CNN\n",
    "3. Makes a direction prediction\n",
    "4. Displays the image with the predicted direction\n",
    "\n",
    "This visualization helps us verify that the model is correctly interpreting visual cues in the images and would steer the robot appropriately when deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize an image and make a prediction\n",
    "def visualize_and_predict(model, image_path, transform):\n",
    "    # Load the image and apply transformation\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Move image tensor to the same device as the model\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    \n",
    "    classes = ['forward', 'left', 'right']\n",
    "    predicted_label = classes[predicted_class.item()]\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Predicted: {predicted_label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Test Prediction\n",
    "\n",
    "Finally, we'll save our trained model so it can be deployed on the robot's hardware. The model is saved in PyTorch's standard format (.pth file), which can be loaded onto the robot's onboard computer.\n",
    "\n",
    "To verify everything works correctly, we'll test the model on a sample image and visualize the prediction. This simulates what would happen when the robot processes an image from its camera during line following.\n",
    "\n",
    "Once deployed, the robot would:\n",
    "1. Capture an image from its camera\n",
    "2. Process it through this CNN model\n",
    "3. Get a direction prediction (forward, left, or right)\n",
    "4. Adjust its motors accordingly to follow the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"follower.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Test the prediction function with a sample image\n",
    "test_image_path = \"images/right/right_318.png\"  # Replace with a valid image path\n",
    "visualize_and_predict(model, test_image_path, transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
