{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5 -- Human Detection \n",
    "In this tutorial, we will use multithreading to capture both color and depth images. We will then employ the YOLOv11 neural network architecture for human detection. To ensure real-time performance, we will utilize the TensorRT deep learning framework. For more details, visit the following website: https://docs.ultralytics.com/guides/nvidia-jetson/#convert-model-to-tensorrt-and-run-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have uploaded two YOLO TensorRT models to the LEARN page: the YOLO11L FP16 version and the YOLO11N FP32 version. The first model, 'YOLO11n.engine', runs at a faster speed but has limited detection accuracy. The second model, 'yolo11l_half.engine', is the FP16 version, as indicated by 'half' in its name.\n",
    "# If you need other versions, please refer to the following link:\n",
    "# https://docs.ultralytics.com/modes/export/#arguments\n",
    "\n",
    "# Below is the code I used to convert the YOLO11L FP16 model\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11l.pt\")  \n",
    "# model.export(format=\"engine\",half=True)  # FP16\n",
    "# Note: Generating the 'yolo11l.engine' file may take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Run YOLOV11 on recorded video data\n",
    "\n",
    "Before running the program, download the pre-trained detection model and the video file 'color_video.avi', then place them in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "#Convert a NumPy array to JPEG-encoded data for display\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "# model = YOLO(\"yolo11n.engine\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"color_video.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the video codec and save the processed video.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # 'mp4v' codec, suitable for MP4 files\n",
    "width, height = 672,376 #VGA resolution\n",
    "fps = 30\n",
    "color_file = cv2.VideoWriter('color_video_processed.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        t1 = cv2.getTickCount()\n",
    "        # Run YOLO inference on the frame\n",
    "        results = model(frame,verbose=False)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "        color_file.write(annotated_frame)\n",
    "\n",
    "        framed = frame.copy()\n",
    "        #set a confidence threshold to filter out unconfident boxes\n",
    "        #https://docs.ultralytics.com/modes/predict/#boxes\n",
    "        conf_threshold = 0.5\n",
    "        for result in results:\n",
    "            #get the human subject\n",
    "            for i in range (len(result.boxes.cls)):\n",
    "                if(result.boxes.cls[i] == 0):  #human subject\n",
    "                    # print(result.boxes.xywh[i])\n",
    "                    if (result.boxes.conf[i] > conf_threshold): #you\n",
    "                        # print()\n",
    "                        bbox = result.boxes.xyxy[i]\n",
    "                        # print(bbox)\n",
    "                        cv2.rectangle(framed, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "                        # cv2.imwrite('human.jpg',color_img)\n",
    "                \n",
    "        scale = 0.3\n",
    "        resized_image = cv2.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        resized_image2 = cv2.resize(framed, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_image)\n",
    "        display_depth.value = bgr8_to_jpeg(resized_image2)\n",
    "        \n",
    "        # total_time = (cv2.getTickCount() - t1) / cv2.getTickFrequency()\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "color_file.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Start the ZED2i Camera system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to load the YOLO model if you skip the first code block.\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "#Start the camera system\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())   \n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform object detection on live video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import motors\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from collections import deque\n",
    "\n",
    "# Initialize the MotorsYukon class for motor control\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "# Initialize tracking history\n",
    "tracking_history = deque(maxlen=5)  \n",
    "\n",
    "def move_forward(speed=0.8):\n",
    "    print(f\"Moving forward at speed {speed}\")\n",
    "    robot.forward(speed)\n",
    "\n",
    "def stop_robot():\n",
    "    print(\"Stopping robot\")\n",
    "    robot.stop()\n",
    "\n",
    "def turn_left(speed=0.5):\n",
    "    print(f\"Turning left at speed {speed}\")\n",
    "    robot.left(speed)\n",
    "\n",
    "def turn_right(speed=0.5):\n",
    "    print(f\"Turning right at speed {speed}\")\n",
    "    robot.right(speed)\n",
    "\n",
    "# Create widgets for displaying the image\n",
    "display_color = widgets.Image(format='jpeg', width='50%')\n",
    "display_depth = widgets.Image(format='jpeg', width='50%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "selected_person = None\n",
    "previous_action = None\n",
    "previous_time = time.time()\n",
    "previous_depth_value = 1500  \n",
    "\n",
    "# Thresholds\n",
    "target_distance = 1700  # Move forward if farther than 1.7m\n",
    "safe_distance = 500  # Stop if closer than 0.9m\n",
    "turn_threshold = 100  # Allow more margin for turns\n",
    "conf_threshold = 0.4  # Confidence threshold for detection\n",
    "\n",
    "# Function to ensure bounding box tracking is consistent\n",
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1b, y1b, x2b, y2b = box2\n",
    "\n",
    "    xi1 = max(x1, x1b)\n",
    "    yi1 = max(y1, y1b)\n",
    "    xi2 = min(x2, x2b)\n",
    "    yi2 = min(y2, y2b)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - y1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2b - x1b) * (y2b - y1b)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area else 0\n",
    "\n",
    "# Convert image to JPEG format for widgets\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# Callback function for human tracking and obstacle avoidance\n",
    "def func(change):\n",
    "    global selected_person, previous_action, previous_time, previous_depth_value\n",
    "    frame = change['new']\n",
    "    results = model(frame, verbose=False)\n",
    "\n",
    "    print(f\"📡 Detected objects: {len(results[0].boxes.cls)}\")  \n",
    "\n",
    "    closest_person = None\n",
    "    min_depth = float('inf')\n",
    "\n",
    "    for result in results:  \n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:  # Human detected\n",
    "                bbox = result.boxes.xyxy[i]\n",
    "                x_center = int((bbox[0] + bbox[2]) / 2)\n",
    "                y_center = int((bbox[1] + bbox[3]) / 2)\n",
    "\n",
    "                if 0 <= y_center < camera.depth_image.shape[0] and 0 <= x_center < camera.depth_image.shape[1]:\n",
    "                    depth_value = camera.depth_image[y_center, x_center]\n",
    "                else:\n",
    "                    depth_value = target_distance  \n",
    "\n",
    "                if depth_value is None or np.isnan(depth_value):\n",
    "                    print(\"⚠️ Invalid depth value detected! Using last known depth.\")\n",
    "                    depth_value = previous_depth_value  \n",
    "\n",
    "                previous_depth_value = depth_value  \n",
    "\n",
    "                print(f\"Human detected at index {i} with confidence {result.boxes.conf[i]} at depth {depth_value} mm\")\n",
    "\n",
    "                # Track the closest person\n",
    "                if depth_value < min_depth and result.boxes.conf[i] > conf_threshold:\n",
    "                    min_depth = depth_value\n",
    "                    closest_person = i  \n",
    "\n",
    "    # Assign the closest detected person as the selected person\n",
    "    if selected_person is None and closest_person is not None:\n",
    "        selected_person = closest_person\n",
    "        tracking_history.append(result.boxes.xyxy[selected_person])\n",
    "        print(f\"First person detected! Tracking person ID: {selected_person}\")\n",
    "\n",
    "    # If the tracked person disappears, reselect using IoU\n",
    "    if selected_person is not None and (selected_person >= len(result.boxes.cls)):\n",
    "        print(\" Lost tracked person! Selecting a new closest person.\")\n",
    "        best_match = -1\n",
    "        best_iou = 0\n",
    "\n",
    "        for i in range(len(result.boxes.cls)):\n",
    "            if result.boxes.cls[i] == 0:\n",
    "                new_bbox = result.boxes.xyxy[i]\n",
    "                avg_iou = sum(iou(new_bbox, old_bbox) for old_bbox in tracking_history) / len(tracking_history)\n",
    "                if avg_iou > best_iou:\n",
    "                    best_iou = avg_iou\n",
    "                    best_match = i\n",
    "\n",
    "        selected_person = best_match if best_match != -1 else closest_person\n",
    "\n",
    "    if selected_person is not None:\n",
    "        bbox = result.boxes.xyxy[selected_person]\n",
    "        x_center = int((bbox[0] + bbox[2]) / 2)\n",
    "        y_center = int((bbox[1] + bbox[3]) / 2)\n",
    "\n",
    "        print(f\"Tracking person depth: {depth_value} mm\")\n",
    "\n",
    "        if depth_value < safe_distance or np.isnan(depth_value):  \n",
    "            print(\"Obstacle detected! Stopping to avoid collision.\")\n",
    "            robot.stop()\n",
    "            return  \n",
    "\n",
    "        if depth_value > target_distance:\n",
    "            print(\"Moving forward towards target\")\n",
    "            robot.forward(0.8)  \n",
    "\n",
    "        frame_center = camera.width // 2\n",
    "        if x_center < frame_center - turn_threshold:\n",
    "            print(\"Turning left to follow target.\")\n",
    "            robot.left(0.5)  \n",
    "\n",
    "        elif x_center > frame_center + turn_threshold:\n",
    "            print(\"Turning right to follow target.\")\n",
    "            robot.right(0.5)  \n",
    "\n",
    "    # Ensure valid depth values (convert NaN to 0)\n",
    "    depth_image = camera.depth_image.copy()\n",
    "    depth_image[np.isnan(depth_image)] = 0  # Convert NaNs to 0\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Update the widgets\n",
    "    scale = 0.3\n",
    "    resized_color = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    display_color.value = bgr8_to_jpeg(resized_color)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "camera.observe(func, names=['color_value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
